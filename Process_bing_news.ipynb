{"cells":[{"cell_type":"markdown","source":["# Read the Json File as a Dataframe"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5950d48f-8c31-49d8-be18-7e3052bf5131"},{"cell_type":"code","source":["df = spark.read.option(\"multiline\", \"true\").json(\"Files/bing-latest-news.json\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":true},"nteract":{"transient":{"deleting":false}},"collapsed":false},"id":"69408263-0eaa-4cb3-b468-a3b6d99951d6"},{"cell_type":"markdown","source":["# Select and explode only the value column from the json"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5ee5af57-b5b7-4d29-a3f2-2c81b47ac309"},{"cell_type":"code","source":["from pyspark.sql.functions import explode\n","def_exploded = df.select(explode(df[\"value\"]).alias(\"json_object\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"3202d8cc-0a6e-4cae-bd16-b6f443eeeb83"},{"cell_type":"markdown","source":["# convert to json"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"5980ad25-e80f-4d01-8dcc-86eb8a059083"},{"cell_type":"code","source":["json_list = def_exploded.toJSON().collect()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"18825e00-e7e1-4357-8ed3-0feac9afbd80"},{"cell_type":"markdown","source":["# Convert the different items into json dictionary and load them into lists"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"ddb4ca73-69d0-42a4-95cd-1c7cd0419c7a"},{"cell_type":"code","source":["import json\n","\n","title = []\n","description = []\n","category = []\n","url = []\n","image = []\n","provider = []\n","datePublished = []\n","\n","for counter_str in json_list:\n","    try:\n","        news_json = json.loads(counter_str)\n","        # no processing if the json from api does not contain category or image\n","        if(not news_json['json_object'].get(\"category\") is None and not news_json['json_object'].get('image') is None):\n","            title.append(news_json['json_object']['name'])\n","            description.append(news_json['json_object']['description'])\n","            category.append(news_json['json_object']['category'])\n","            url.append(news_json['json_object']['url'])\n","            image.append(news_json['json_object']['image']['thumbnail']['contentUrl'])\n","            provider.append(news_json['json_object']['provider'][0]['name'])\n","            datePublished.append(news_json['json_object']['datePublished'])\n","    except Exception as e:\n","        print(f\"errors processing json: {e}\")\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"7e86b124-d0c3-410e-9652-40a93faef282"},{"cell_type":"markdown","source":["# Combine the different lists into a Dataframe "],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"28e722a4-4504-4e74-9872-624b0aa35925"},{"cell_type":"code","source":["from pyspark.sql.types import StructType,StructField, StringType\n","\n","data = list(zip(title,description,category,url,image,provider,datePublished))\n","\n","schema = StructType([\n","    StructField(\"title\", StringType(), True),\n","    StructField(\"description\", StringType(), True),\n","    StructField(\"category\", StringType(), True),\n","    StructField(\"url\", StringType(), True),\n","    StructField(\"image\", StringType(), True),\n","    StructField(\"provider\", StringType(), True),\n","    StructField(\"datePublished\", StringType(), True)\n","    ])\n","\n","df_cleaned = spark.createDataFrame(data,schema =schema)\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"915da1b3-f20b-4e5a-afd5-1a97b9a6ef0a"},{"cell_type":"markdown","source":["# Convert date"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"a39d6fd8-1efe-4f51-92f4-c5778d43d415"},{"cell_type":"code","source":["from pyspark.sql.functions import to_date, date_format\n","df_cleaned_final = df_cleaned.withColumn(\"datePublished\", date_format(to_date(\"datePublished\"), \"dd-MM-yyyy\"))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"fbaaed20-0c4e-4f57-bee6-c265817ac84b"},{"cell_type":"markdown","source":["# Writing the Final Dataframe to the Lakehouse DB in a Delta format using INCREMENTAL LOAD Type 1"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"fe616de0-96aa-483a-8282-22b08b6c01f9"},{"cell_type":"code","source":["from pyspark.sql.utils import AnalysisException\n","\n","table_name = \"bing_lake_db.tbl_latest_news\"\n","\n","try: \n","    df_cleaned_final.write.format(\"delta\").saveAsTable(table_name)\n","\n","except AnalysisException:\n","    print (\"Table already exists!\")\n","\n","    df_cleaned_final.createOrReplaceTempView(\"vw_df_cleaned_final\")\n","    # check if the url matched between the source and the target, if it matched then, check if any of the \n","    # column values has changed and update the whole row accordingly, if not insert the whole row in the table\n","    spark.sql(f\"\"\"MERGE INTO {table_name} target_table\n","                     USING vw_df_cleaned_final source_view\n","                     ON source_view.url = target_table.url\n","\n","                     WHEN MATCHED AND\n","                     (source_view.title <> target_table.title OR\n","                     source_view.description <> target_table.description OR\n","                     source_view.category <> target_table.category OR\n","                     source_view.image <> target_table.image OR\n","                     source_view.provider <> target_table.provider OR\n","                     source_view.datePublished <> target_table.datePublished)\n","\n","                     THEN UPDATE SET *\n","                     WHEN NOT MATCHED THEN INSERT *\n","\n","                \"\"\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"session_id":null,"statement_id":null,"state":"cancelled","livy_statement_state":null,"queued_time":"2024-04-23T10:27:31.0027044Z","session_start_time":null,"execution_start_time":null,"execution_finish_time":"2024-04-23T10:27:32.0247248Z","parent_msg_id":"51575234-625d-45ea-8ab6-3ec47d8710aa"},"text/plain":"StatementMeta(, , , Cancelled, )"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}}},"id":"427a9939-95d7-4051-a029-4afa0e08dc6c"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"2f9dd691-a1ea-4e81-9106-9332af35aaa3","default_lakehouse_name":"bing_lake_db","default_lakehouse_workspace_id":"66da6725-788c-44a2-bfb8-99593efea413"}}},"nbformat":4,"nbformat_minor":5}